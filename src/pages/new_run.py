# src/pages/new_run.py
# v7.2 â€“ final bug-free per-folder & equal-count sliders
from __future__ import annotations

import time
from pathlib import Path
from typing import Dict, List

import streamlit as st

from src.pages.utils import log, show_log
from src.pipeline import run_test
from src.results_db import save_run, load_run
from src.models import MODEL_REGISTRY

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ project root â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = Path(__file__).resolve().parents[2]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• helpers â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def _folder_doc_counts(paths: Dict[str, str]) -> Dict[str, int]:
    """Return {folder-label: number_of_docx}."""
    return {
        label: len(list((ROOT / path).glob("*.docx")))
        for label, path in paths.items()
    }


def _select_documents(
    folders: List[str],
    limits: Dict[str, int],
) -> Dict[str, int]:
    """
    Render sliders and return {folder: docs_to_include}.
    Order: â‘  global equal-count (if applicable) âœ â‘¡ per-folder sliders.
    """
    counts: Dict[str, int] = {}

    # Store for possible later use
    st.session_state["selected_folders"] = folders

    # â”€â”€ 1 Â· global equal-count (only when 2+ folders) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if len(folders) >= 2:
        min_available = min(limits[f] for f in folders)
        default_val = st.session_state.get("equal_count", min_available)

        equal_val = st.slider(
            "ğŸ”„ Equal docs for *all* selected folders",
            min_value=1,
            max_value=min_available,
            value=default_val,
            key="equal_count",
            help="Drag to assign the same document count to every folder",
        )

        # Detect change since last run *before* per-folder sliders exist
        prev = st.session_state.get("prev_equal_val")
        if prev is None or prev != equal_val:
            st.session_state["prev_equal_val"] = equal_val
            # Push into per-folder counts (safe â€“ widgets not created yet)
            for lbl in folders:
                st.session_state[f"count_{lbl}"] = equal_val

    # â”€â”€ 2 Â· per-folder sliders â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("ğŸ“‚ Documents per folder")

    for lbl in folders:
        max_docs = limits[lbl]
        key = f"count_{lbl}"
        # Initialise once with â€œallâ€ if not yet present
        if key not in st.session_state:
            st.session_state[key] = max_docs

        counts[lbl] = st.slider(
            f"{lbl} â€“ documents to include",
            min_value=1,
            max_value=max_docs,
            value=min(st.session_state[key], max_docs),
            key=key,
            help=f"{max_docs} .docx files available",
        )

    st.divider()
    return counts


def _gather_docs(selected: Dict[str, int], paths: Dict[str, str]) -> List[Path]:
    """Return actual Path list based on selected counts."""
    out: List[Path] = []
    for lbl, n in selected.items():
        folder = ROOT / paths[lbl]
        out.extend(sorted(folder.glob("*.docx"))[: n])
    return out


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• PAGE â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def page_new_run():
    st.header("âš¡ï¸ Launch new benchmark")

    with st.expander("â„¹ï¸ About Benchmarking", expanded=False):
        st.markdown(
            """
            **What this does**

            * Tests multiple humanizer models on your documents  
            * Measures AI-detection scores **before & after** humanization  
            * Evaluates content-quality preservation  
            * Runs several iterations for robustness
            """
        )

    # â”€â”€ 1 Â· run name â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    run_name = st.text_input(
        "Unique run name",
        placeholder="Enter a descriptive name for this benchmark run",
    )

    # â”€â”€ 2 Â· folder selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    FOLDERS = {
        "AI texts": "data/ai_texts",
        "Human texts": "data/human_texts",
        "Mixed texts": "data/mixed_texts",
    }
    folder_labels = st.multiselect(
        "Folders to include",
        list(FOLDERS),
        help="Pick one or more folders",
    )

    if not folder_labels:
        st.info("Pick at least one folder to continue")
        return

    limits = _folder_doc_counts({f: FOLDERS[f] for f in folder_labels})
    doc_counts = _select_documents(folder_labels, limits)

    # â”€â”€ 3 Â· model selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_models = list(MODEL_REGISTRY)
    model_labels = st.multiselect(
        "Humanizer models",
        all_models,
        default=all_models[:3],
        help="Select which models you wish to test",
    )

    # â”€â”€ 4 Â· iteration count â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    iterations = st.slider(
        "Iterations per document",
        1,
        10,
        value=5,
        help="How many drafts each model should generate for every document",
    )

    # â”€â”€ 5 Â· workload preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if model_labels:
        docs = _gather_docs(doc_counts, FOLDERS)
        total_drafts = len(docs) * len(model_labels) * iterations * 2
        st.info(
            f"ğŸ“Š **Workload preview:** {len(docs)} docs Ã— "
            f"{len(model_labels)} models Ã— {iterations} iterations Ã— 2 modes "
            f"= **{total_drafts} drafts**"
        )

    # â”€â”€ 6 Â· live log placeholder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    log_box = st.empty()

    # â”€â”€ 7 Â· RUN button â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if st.button(
        "ğŸš€ Run benchmark",
        type="primary",
        disabled=not (run_name.strip() and folder_labels and model_labels),
    ):
        if load_run(run_name):
            st.error("Run name already exists")
            st.stop()

        docs = _gather_docs(doc_counts, FOLDERS)
        if not docs:
            st.error("No .docx files found for the current settings")
            st.stop()

        # â”€â”€ initial log â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        log(f"ğŸš€ Benchmark **{run_name}**")
        log(
            "ğŸ“‚ Folders: "
            + ", ".join(f"{f}({doc_counts[f]})" for f in folder_labels)
        )
        log(f"ğŸ¤– Models:  {', '.join(model_labels)}")
        log(f"ğŸ” Iterations: {iterations}")
        log(f"ğŸ“ Total drafts: {len(docs)*len(model_labels)*iterations*2}")
        show_log(log_box)

        # â”€â”€ processing loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        with st.status("Running benchmark â€¦", expanded=True) as status:
            t0 = time.time()
            results = []

            for idx, doc_path in enumerate(docs, 1):
                status.update(label=f"{idx}/{len(docs)} â€“ {doc_path.name}")
                st.progress(idx / len(docs))
                log(f"\nğŸ“„ ({idx}/{len(docs)}) {doc_path.name}")
                show_log(log_box)

                try:
                    res = run_test(doc_path, model_labels, log, iterations)
                    if res.get("runs"):
                        results.append(res)
                        log(f"âœ… Completed {doc_path.name} â€“ {len(res['runs'])} drafts")
                    else:
                        log(f"âš ï¸  Skipped {doc_path.name} (no paragraphs)")
                except Exception as e:
                    log(f"âŒ ERROR in {doc_path.name}: {e}")

                show_log(log_box)

            # â”€â”€ save run â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            duration = (time.time() - t0) / 60
            log(f"\nğŸ’¾ Saving run **{run_name}** (took {duration:.1f} min)")
            save_run(
                run_name,
                folder_labels,
                model_labels,
                {"docs": results, "iterations": iterations, "doc_counts": doc_counts},
            )
            show_log(log_box)
            status.update("Benchmark finished!", state="complete", expanded=False)

        # â”€â”€ summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        st.success(f"âœ… Run **{run_name}** completed in {duration:.1f} minutes")

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ğŸ“„ Documents", len(results))
        with col2:
            st.metric("ğŸ“ Drafts", sum(len(d["runs"]) for d in results))
        with col3:
            st.metric("ğŸ¤– Models", len(model_labels))
        with col4:
            avg_sec = (duration * 60) / len(results) if results else 0
            st.metric("â±ï¸ Avg time/doc", f"{avg_sec:.1f}s")
